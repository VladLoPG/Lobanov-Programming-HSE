{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Скачаем датасет для задачи суммаризации статей на английском"
      ],
      "metadata": {
        "id": "uhbsOtXCcuS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "path = kagglehub.dataset_download(\"evilspirit05/daily-mail-summarization-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzdhNzU_CXRN",
        "outputId": "8d1827d0-18dc-4e4b-a8cc-9dac90619118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/daily-mail-summarization-dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Импортируем нужные библиотеки"
      ],
      "metadata": {
        "id": "G0e4qwpTc0Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "\n",
        "set_global_policy('mixed_float16')  # Enable mixed precision\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "AUxm4KezLD03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Загружаем датасет в датафрейм"
      ],
      "metadata": {
        "id": "9fSkCPAxdAeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(path + '/article_highlights.csv', low_memory=False)"
      ],
      "metadata": {
        "id": "2oL0z9P8LPax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Функция для простой предобработки текста, нам больше и не нужно, текст уже достаточно чистый"
      ],
      "metadata": {
        "id": "aZqK5E9_dILa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    text = \" \".join(words)\n",
        "    return text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWgdk8JFBYNb",
        "outputId": "f15d77a0-98b8-4a20-c86d-005c8cb153fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проведем предобработку и убедимся, что все значения в колонке строкового типа (у меня были с этим проблемы)"
      ],
      "metadata": {
        "id": "vUNPMdgadV86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['article'] = df['article'].astype(str).apply(preprocess_text)\n",
        "df['highlights'] = df['highlights'].astype(str).apply(preprocess_text)"
      ],
      "metadata": {
        "id": "MpxFVB98BeMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Присвоим значения двум переменным, одна из них - инпут, другая - таргет текст"
      ],
      "metadata": {
        "id": "bsEMOcTJdQmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts = df[\"article\"].values\n",
        "target_texts = df[\"highlights\"].values"
      ],
      "metadata": {
        "id": "A4BcoZnuj5dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Определим параметры будущей модели"
      ],
      "metadata": {
        "id": "sieqUZlAdOqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры\n",
        "max_input_length = 100  # Максимальная длина входной последовательности\n",
        "max_target_length = 50  # Максимальная длина выходной последовательности\n",
        "vocab_size = 10000  # Размер словаря\n",
        "embedding_dim = 256  # Размерность эмбеддингов\n",
        "lstm_units = 512  # Размерность слоя LSTM"
      ],
      "metadata": {
        "id": "ZITLanQWK9ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Проведем токенизацию + настроим слои модели"
      ],
      "metadata": {
        "id": "oGFml37JdcX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Токенизируем тексты\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(np.concatenate([input_texts, target_texts]))\n",
        "\n",
        "# Переводим тексты в последовательность целых чисел\n",
        "input_sequences = tokenizer.texts_to_sequences(input_texts)\n",
        "target_sequences = tokenizer.texts_to_sequences(target_texts)\n",
        "\n",
        "# Добавляем токены выходной последовательности\n",
        "start_token = vocab_size  # Assign a unique token for <start>\n",
        "end_token = vocab_size + 1  # Assign a unique token for <end>\n",
        "\n",
        "target_sequences_in = [[start_token] + seq for seq in target_sequences]  # Вход декодеру\n",
        "target_sequences_out = [seq + [end_token] for seq in target_sequences]  # Выход из декодера\n",
        "\n",
        "# Паддинг\n",
        "input_data = pad_sequences(input_sequences, maxlen=max_input_length, padding=\"post\")\n",
        "target_data_in = pad_sequences(target_sequences_in, maxlen=max_target_length, padding=\"post\")\n",
        "target_data_out = pad_sequences(target_sequences_out, maxlen=max_target_length, padding=\"post\")\n",
        "\n",
        "# Определим энкодер\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = Embedding(input_dim=vocab_size + 2, output_dim=embedding_dim)(encoder_inputs)\n",
        "encoder_lstm = LSTM(lstm_units, return_state=True, dropout=0.2, recurrent_dropout=0.2, recurrent_regularizer=l2(0.01))\n",
        "_, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "# Определим декодер\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = Embedding(input_dim=vocab_size + 2, output_dim=embedding_dim)(decoder_inputs)\n",
        "decoder_lstm = LSTM(lstm_units, return_sequences=True, return_state=True, dropout=0.2, recurrent_dropout=0.2, recurrent_regularizer=l2(0.01))\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(vocab_size + 2, activation=\"softmax\")\n",
        "output = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "E26vtMqOPYVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Компилируем модель"
      ],
      "metadata": {
        "id": "Ep8njB4Xdh3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Определим модель\n",
        "model = Model([encoder_inputs, decoder_inputs], output)\n",
        "\n",
        "\n",
        "# Компиляция\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "O0Z5DX86RgLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Обучение модели"
      ],
      "metadata": {
        "id": "cHQm-etudkA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Последние приготовления\n",
        "target_data_out_one_hot = np.expand_dims(target_data_out, -1)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "\n",
        "model.fit(\n",
        "    [input_data, target_data_in[:, :-1]],\n",
        "    target_data_out_one_hot[:, 1:],\n",
        "    batch_size=128,\n",
        "    epochs=30,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCUdZIYOQp1S",
        "outputId": "ea072984-7331-4a5d-8659-aed8de389998"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 941ms/step - accuracy: 0.6582 - loss: 10.2635 - val_accuracy: 0.6913 - val_loss: 3.1850\n",
            "Epoch 2/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 888ms/step - accuracy: 0.7185 - loss: 1.6257 - val_accuracy: 0.6981 - val_loss: 2.8905\n",
            "Epoch 3/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 889ms/step - accuracy: 0.7446 - loss: 1.1952 - val_accuracy: 0.7147 - val_loss: 2.9388\n",
            "Epoch 4/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 970ms/step - accuracy: 0.8718 - loss: 0.7995 - val_accuracy: 0.7418 - val_loss: 2.9018\n",
            "Epoch 5/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 884ms/step - accuracy: 0.9419 - loss: 0.4747 - val_accuracy: 0.7580 - val_loss: 2.8454\n",
            "Epoch 6/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 885ms/step - accuracy: 0.9633 - loss: 0.2994 - val_accuracy: 0.7521 - val_loss: 2.8057\n",
            "Epoch 7/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 965ms/step - accuracy: 0.9719 - loss: 0.2014 - val_accuracy: 0.7559 - val_loss: 2.7876\n",
            "Epoch 8/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 887ms/step - accuracy: 0.9753 - loss: 0.1461 - val_accuracy: 0.7551 - val_loss: 2.7862\n",
            "Epoch 9/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 891ms/step - accuracy: 0.9768 - loss: 0.1220 - val_accuracy: 0.7559 - val_loss: 2.7841\n",
            "Epoch 10/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 876ms/step - accuracy: 0.9777 - loss: 0.1073 - val_accuracy: 0.7596 - val_loss: 2.7799\n",
            "Epoch 11/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 873ms/step - accuracy: 0.9785 - loss: 0.0976 - val_accuracy: 0.7596 - val_loss: 2.7821\n",
            "Epoch 12/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 874ms/step - accuracy: 0.9790 - loss: 0.0921 - val_accuracy: 0.7596 - val_loss: 2.7840\n",
            "Epoch 13/30\n",
            "\u001b[1m58/58\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 872ms/step - accuracy: 0.9795 - loss: 0.0880 - val_accuracy: 0.7588 - val_loss: 2.7879\n",
            "Model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Код для проверки работы модели. Спойлер - не очень..."
      ],
      "metadata": {
        "id": "fsz5Ubeadm4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_model = Model(inputs=model.input[0], outputs=encoder_states)\n",
        "decoder_state_input_h = Input(shape=(lstm_units,))\n",
        "decoder_state_input_c = Input(shape=(lstm_units,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_embedding, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
        "\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    target_seq = np.zeros((1, 1))\n",
        "    target_seq[0, 0] = start_token\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = tokenizer.index_word.get(sampled_token_index, \"\")\n",
        "\n",
        "        if sampled_char == \"\" or sampled_token_index == end_token:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            decoded_sentence += sampled_char + \" \"\n",
        "\n",
        "        target_seq = np.zeros((1, 1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence.strip()\n",
        "\n",
        "\n",
        "def summarize_text(text):\n",
        "    input_seq = tokenizer.texts_to_sequences([text])\n",
        "    input_seq = pad_sequences(input_seq, maxlen=max_input_length, padding=\"post\")\n",
        "    summary = decode_sequence(input_seq)\n",
        "    return summary\n",
        "\n",
        "# Применим на практике\n",
        "text_to_summarize = '''Marco Rubio, the secretary of state, spoke on Tuesday with his Hungarian counterpart, the foreign minister, Péter Szijjártó, and informed him of the move, state department spokesperson Tammy Bruce said in a statement.\n",
        "\n",
        "“The Secretary informed foreign minister Szijjártó of senior Hungarian official Antal Rogán’s removal from the US Department of the Treasury’s Specially Designated Nationals and Blocked Persons List, noting that continued designation was inconsistent with US foreign policy interests,” Bruce said.\n",
        "\n",
        "The two also discussed strengthening US-Hungary alignment on critical issues and opportunities for economic cooperation, Bruce said.\n",
        "\n",
        "Rogán is a close aide of Orbán and has run his cabinet office since 2015.'''  # Replace with the actual text\n",
        "summary = summarize_text(text_to_summarize)\n",
        "summary\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "_2fLzns8_XJq",
        "outputId": "a424dbfd-2d69-4400-b6b1-dfadd14aedf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 893ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 374ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bush london london'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Люблю рнн... (может, я ее не смог понять, но работать нормально не заставил)"
      ],
      "metadata": {
        "id": "OXk6ZYEBefgy"
      }
    }
  ]
}